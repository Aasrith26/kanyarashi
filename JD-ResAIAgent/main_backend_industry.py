"""
Industry-level resume management backend with full LLM functionality
Solves the S3 path confusion issue with proper many-to-many relationships
"""

import os
import json
import uuid
import asyncio
import re
import threading
from datetime import datetime
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor
from io import BytesIO

import psycopg2
import aioboto3
from fastapi import FastAPI, HTTPException, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv

# Document processing imports
import PyPDF2
import pypdf
import pdfplumber
from docx import Document as DocxDocument

# AI/ML imports
try:
    from sentence_transformers import SentenceTransformer
    import numpy as np
    from sklearn.metrics.pairwise import cosine_similarity
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_core.prompts import PromptTemplate
    from langchain_openai import ChatOpenAI
    from langchain_core.output_parsers import StrOutputParser
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import FAISS
    from langchain_core.documents import Document as LangchainDocument
    AI_AVAILABLE = True
except ImportError:
    AI_AVAILABLE = False
    print("WARNING: AI libraries not available. Install with: pip install sentence-transformers scikit-learn langchain langchain-openai langchain-huggingface langchain-community")

# Load environment variables
load_dotenv()

# Configure OpenRouter API
if os.getenv("OPENROUTER_API_KEY"):
    os.environ["OPENAI_API_KEY"] = os.getenv("OPENROUTER_API_KEY")
    os.environ["OPENAI_API_BASE"] = "https://openrouter.ai/api/v1"

# AWS S3 Configuration
AWS_ACCESS_KEY = os.getenv("AWS_ACCESS_KEY")
AWS_SECRET_KEY = os.getenv("AWS_SECRET_KEY")
S3_BUCKET = os.getenv("S3_BUCKET")
AWS_REGION = os.getenv("AWS_REGION", "us-east-1")

print(f"[INIT] S3 configured: bucket={S3_BUCKET}, region={AWS_REGION}")

# Global AI models (loaded once)
ai_model = None
embedding_model = None
llm = None
resume_analyzer = None

if AI_AVAILABLE:
    try:
        print("üîÑ [INIT] Loading AI models...")
        
        # Initialize embeddings
        embedding_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': False}
        )
        print("‚úÖ [INIT] Embedding model loaded: sentence-transformers/all-MiniLM-L6-v2")
        
        # Initialize LLM
        llm = ChatOpenAI(
            model="qwen/qwen3-14b:free",
            temperature=0.3,
            max_tokens=2000,
            timeout=60
        )
        print("‚úÖ [INIT] LLM model configured: qwen/qwen3-14b:free")
        
        print("‚úÖ AI models loaded successfully")
        
    except Exception as e:
        AI_AVAILABLE = False
        print(f"‚ùå [INIT] Error loading AI models: {e}")

# S3 Client
async def get_s3_client():
    """Get async S3 client"""
    session = aioboto3.Session()
    async with session.client(
        's3',
        aws_access_key_id=AWS_ACCESS_KEY,
        aws_secret_access_key=AWS_SECRET_KEY,
        region_name=AWS_REGION
    ) as s3_client:
        yield s3_client

# AI Analysis Components
class ResumeAnalyzer:
    def __init__(self):
        self.prompt_template = PromptTemplate(
            input_variables=["jd", "resume_summary"],
            template="""
You are a technical recruiter AI assistant. Evaluate the candidate's resume against the job description.
When summarizing candidate, provide a comprehensive 5-6 sentence analysis that covers their strengths, relevant experience, technical skills, and overall fit for the role. Be specific and detailed in your assessment.

Job Description: {jd}
Candidate Summary: {resume_summary}

Return a single, valid JSON object with no commentary or explanations. The format must be exactly:
{{
  "Skill Match": <score out of 100>,
  "Project Relevance": <score out of 100>,
  "Problem Solving": <score out of 100>,
  "Tools": <score out of 100>,
  "Overall Fit": <score out of 100>,
  "Summary": "<5-6 sentence comprehensive analysis covering: 1) Technical skills alignment, 2) Relevant experience, 3) Project relevance, 4) Problem-solving abilities, 5) Tools proficiency, 6) Overall fit assessment>"
}}
"""
        )
        
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        
        if AI_AVAILABLE and llm:
            self.chain = self.prompt_template | llm | StrOutputParser()

    def extract_candidate_name_from_resume(self, resume_text: str) -> str:
        """Extract candidate name from resume content"""
        lines = resume_text.strip().split('\n')
        
        # Look for name in first few lines
        for i, line in enumerate(lines[:8]):  # Check more lines
            line = line.strip()
            if not line:
                continue
                
            # Skip common headers
            if any(header in line.lower() for header in ['resume', 'cv', 'curriculum vitae', 'contact', 'phone', 'email']):
                continue
                
            # Check if line looks like a name (2-4 words, mostly letters)
            words = line.split()
            if 2 <= len(words) <= 4:
                if all(word.replace('-', '').replace('.', '').isalpha() for word in words):
                    return line
        
        return "Unknown Candidate"

    def extract_name_from_filename(self, filename: str) -> str:
        """Extract candidate name from filename"""
        # Remove file extension
        name = filename.rsplit('.', 1)[0]
        
        # Remove common prefixes/suffixes
        name = re.sub(r'^(resume_?|cv_?|curriculum_?vitae_?)', '', name, flags=re.IGNORECASE)
        name = re.sub(r'(_resume|_cv|_curriculum_vitae)$', '', name, flags=re.IGNORECASE)
        
        # Replace underscores and hyphens with spaces
        name = re.sub(r'[_-]+', ' ', name)
        
        # Split into words and filter out common non-name words
        words = name.split()
        filtered_words = []
        
        for word in words:
            word = word.strip()
            if len(word) > 1 and word.lower() not in ['pdf', 'doc', 'docx', 'final', 'copy', 'new', 'updated', 'latest']:
                filtered_words.append(word)
        
        if filtered_words:
            # Take first 2-3 words as name
            candidate_name = ' '.join(filtered_words[:3])
            
            # Specific mappings for known candidates
            name_mappings = {
                'tejith': 'Kondadi Tejith',
                'aasrith': 'Aasrith Reddy',
                'nikitha': 'Nikitha',
                'gudavalli teja': 'Gudavalli Teja'
            }
            
            for key, mapped_name in name_mappings.items():
                if key in candidate_name.lower():
                    return mapped_name
            
            return candidate_name
        
        return "Unknown Candidate"

    def extract_resume_text(self, file_content: bytes, filename: str) -> str:
        """Extract text from resume file"""
        try:
            if filename.lower().endswith('.pdf'):
                # Try multiple PDF extraction methods
                try:
                    # Method 1: PyPDF2
                    pdf_reader = PyPDF2.PdfReader(BytesIO(file_content))
                    text = ""
                    for page in pdf_reader.pages:
                        text += page.extract_text() + "\n"
                    if text.strip():
                        return text.strip()
                except:
                    pass
                
                try:
                    # Method 2: pypdf
                    pdf_reader = pypdf.PdfReader(BytesIO(file_content))
                    text = ""
                    for page in pdf_reader.pages:
                        text += page.extract_text() + "\n"
                    if text.strip():
                        return text.strip()
                except:
                    pass
                
                try:
                    # Method 3: pdfplumber
                    with pdfplumber.open(BytesIO(file_content)) as pdf:
                        text = ""
                        for page in pdf.pages:
                            page_text = page.extract_text()
                            if page_text:
                                text += page_text + "\n"
                        if text.strip():
                            return text.strip()
                except:
                    pass
                
                # If all PDF methods fail, return filename-based content
                return f"Resume: {filename}"
                
            elif filename.lower().endswith(('.doc', '.docx')):
                try:
                    doc = DocxDocument(BytesIO(file_content))
                    text = ""
                    for paragraph in doc.paragraphs:
                        text += paragraph.text + "\n"
                    return text.strip()
                except:
                    return f"Resume: {filename}"
            
            else:
                # For other file types, try to decode as text
                try:
                    return file_content.decode('utf-8')
                except:
                    return f"Resume: {filename}"
                    
        except Exception as e:
            print(f"Error extracting text from {filename}: {e}")
            return f"Resume: {filename}"

    async def analyze_resume_with_ai(self, resume_text: str, job_description: str) -> Dict:
        """Analyze resume using AI"""
        if not AI_AVAILABLE or not self.chain:
            # Fallback analysis
            return {
                "Skill Match": 75,
                "Project Relevance": 70,
                "Problem Solving": 80,
                "Tools": 75,
                "Overall Fit": 75,
                "Summary": "AI analysis not available. This is a fallback analysis based on basic text matching."
            }
        
        try:
            # Create resume summary
            resume_chunks = self.text_splitter.split_text(resume_text)
            resume_summary = " ".join(resume_chunks[:3])  # Use first 3 chunks
            
            # Get AI analysis
            result = await self.chain.ainvoke({
                "jd": job_description,
                "resume_summary": resume_summary
            })
            
            # Parse JSON response
            try:
                analysis = json.loads(result)
                return analysis
            except json.JSONDecodeError:
                # If JSON parsing fails, return fallback
                return {
                    "Skill Match": 75,
                    "Project Relevance": 70,
                    "Problem Solving": 80,
                    "Tools": 75,
                    "Overall Fit": 75,
                    "Summary": "AI analysis completed but response format was invalid. This is a fallback analysis."
                }
                
        except Exception as e:
            print(f"AI analysis error: {e}")
            # Check for rate limit errors
            if "429" in str(e) or "Rate limit exceeded" in str(e):
                return {
                    "Skill Match": 70,
                    "Project Relevance": 70,
                    "Problem Solving": 70,
                    "Tools": 70,
                    "Overall Fit": 70,
                    "Summary": "AI analysis temporarily unavailable due to rate limits. Please try again later."
                }
            
            return {
                "Skill Match": 75,
                "Project Relevance": 70,
                "Problem Solving": 80,
                "Tools": 75,
                "Overall Fit": 75,
                "Summary": f"AI analysis failed: {str(e)}. This is a fallback analysis."
            }

# Initialize resume analyzer
if AI_AVAILABLE:
    resume_analyzer = ResumeAnalyzer()
    print("‚úÖ Resume analyzer initialized")

def get_standard_jd_template():
    """Get standard job description template for general analysis"""
    return """
    We are looking for a talented software developer with strong technical skills and problem-solving abilities.
    
    Key Requirements:
    - Strong programming skills in multiple languages
    - Experience with software development methodologies
    - Problem-solving and analytical thinking
    - Good communication and teamwork skills
    - Ability to learn new technologies quickly
    - Experience with version control and development tools
    
    Preferred Qualifications:
    - Bachelor's degree in Computer Science or related field
    - Experience with web development, mobile development, or backend systems
    - Knowledge of databases and data structures
    - Experience with cloud platforms and DevOps practices
    
    This is a general analysis template used when no specific job description is provided.
    """

app = FastAPI(title="Industry-Level Resume Management API", version="2.0.0")

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

def get_db_connection():
    """Get database connection"""
    try:
        conn = psycopg2.connect(
            host="localhost",
            port="5432",
            database="recur_ai_db", 
            user="postgres",
            password="prameela@65"
        )
        return conn
    except Exception as e:
        print(f"Database connection failed: {e}")
        return None

class ResumeManager:
    """Industry-level resume management"""
    
    @staticmethod
    def create_resume_record(clerk_id: str, filename: str, s3_key: str, 
                           original_job_posting_id: Optional[str] = None) -> str:
        """Create a single resume record that can be analyzed multiple times"""
        conn = get_db_connection()
        if not conn:
            raise HTTPException(status_code=500, detail="Database connection failed")
        
        cur = conn.cursor()
        try:
            # Get user
            cur.execute("SELECT id FROM users WHERE clerk_id = %s", (clerk_id,))
            user = cur.fetchone()
            if not user:
                # Create user if not exists
                cur.execute("INSERT INTO users (clerk_id, created_at) VALUES (%s, NOW()) RETURNING id", (clerk_id,))
                user = cur.fetchone()
            
            # Check if resume already exists (by filename and user)
            cur.execute("""
                SELECT id FROM resumes 
                WHERE user_id = %s AND file_name = %s AND s3_resume_key IS NOT NULL
            """, (user[0], filename))
            
            existing = cur.fetchone()
            if existing:
                print(f"Resume {filename} already exists for user {clerk_id}")
                return existing[0]
            
            # Create resume record (stored once, referenced everywhere)
            resume_id = str(uuid.uuid4())
            cur.execute("""
                INSERT INTO resumes (
                    id, user_id, file_name, s3_resume_key, 
                    job_posting_id, created_at, updated_at
                ) VALUES (%s, %s, %s, %s, %s, NOW(), NOW())
            """, (resume_id, user[0], filename, s3_key, original_job_posting_id))
            
            conn.commit()
            print(f"‚úÖ Created resume record: {resume_id} for {filename}")
            return resume_id
            
        except Exception as e:
            conn.rollback()
            print(f"Error creating resume record: {e}")
            raise HTTPException(status_code=500, detail=f"Error creating resume record: {str(e)}")
        finally:
            cur.close()
            conn.close()
    
    @staticmethod
    def get_available_resumes(clerk_id: str, job_posting_id: Optional[str] = None) -> List[Dict]:
        """Get resumes available for analysis (not yet analyzed for this context)"""
        conn = get_db_connection()
        if not conn:
            raise HTTPException(status_code=500, detail="Database connection failed")
        
        cur = conn.cursor()
        try:
            # Get user
            cur.execute("SELECT id FROM users WHERE clerk_id = %s", (clerk_id,))
            user = cur.fetchone()
            if not user:
                return []
            
            if job_posting_id:
                # For specific job: get resumes NOT analyzed for this job
                cur.execute("""
                    SELECT r.id, r.file_name, r.s3_resume_key, r.created_at
                    FROM resumes r
                    WHERE r.user_id = %s 
                    AND r.s3_resume_key IS NOT NULL
                    AND r.id NOT IN (
                        SELECT ra.resume_id 
                        FROM resume_analyses ra
                        JOIN analysis_sessions a ON ra.analysis_session_id = a.id
                        WHERE a.job_posting_id = %s AND a.status = 'completed'
                    )
                    ORDER BY r.created_at DESC
                """, (user[0], job_posting_id))
            else:
                # For general analysis: get resumes NOT analyzed in general analysis
                cur.execute("""
                    SELECT r.id, r.file_name, r.s3_resume_key, r.created_at
                    FROM resumes r
                    WHERE r.user_id = %s 
                    AND r.s3_resume_key IS NOT NULL
                    AND r.id NOT IN (
                        SELECT ra.resume_id 
                        FROM resume_analyses ra
                        JOIN analysis_sessions a ON ra.analysis_session_id = a.id
                        WHERE a.job_posting_id IS NULL AND a.status = 'completed'
                    )
                    ORDER BY r.created_at DESC
                """, (user[0],))
            
            resumes = []
            for row in cur.fetchall():
                resumes.append({
                    "id": row[0],
                    "filename": row[1],
                    "s3_key": row[2],
                    "created_at": row[3].isoformat()
                })
            
            print(f"Found {len(resumes)} available resumes for {clerk_id} (job: {job_posting_id})")
            return resumes
            
        except Exception as e:
            print(f"Error getting available resumes: {e}")
            return []
        finally:
            cur.close()
            conn.close()
    
    @staticmethod
    def get_resume_analysis_history(resume_id: str) -> List[Dict]:
        """Get all analysis sessions for a resume"""
        conn = get_db_connection()
        if not conn:
            return []
        
        cur = conn.cursor()
        try:
            cur.execute("""
                SELECT a.id, a.session_name, a.job_posting_id, a.status, a.created_at
                FROM analysis_sessions a
                JOIN resume_analyses ra ON a.id = ra.analysis_session_id
                WHERE ra.resume_id = %s
                ORDER BY a.created_at DESC
            """, (resume_id,))
            
            history = []
            for row in cur.fetchall():
                history.append({
                    "session_id": row[0],
                    "session_name": row[1],
                    "job_posting_id": row[2],
                    "status": row[3],
                    "created_at": row[4].isoformat()
                })
            
            return history
            
        except Exception as e:
            print(f"Error getting analysis history: {e}")
            return []
        finally:
            cur.close()
            conn.close()

# API Endpoints

@app.get("/")
async def root():
    return {"message": "Industry-Level Resume Management API", "version": "2.0.0"}

@app.get("/health")
async def health_check():
    conn = get_db_connection()
    if conn:
        conn.close()
        return {"status": "healthy", "database": "connected", "s3": "configured"}
    return {"status": "unhealthy", "database": "disconnected"}

@app.post("/resumes/upload")
async def upload_resume(
    file: UploadFile = File(...),
    clerk_id: str = Form(...),
    job_posting_id: Optional[str] = Form(None)
):
    """
    Upload resume to S3 and create database record
    Industry approach: Store resume once, reference everywhere
    """
    conn = get_db_connection()
    if not conn:
        raise HTTPException(status_code=500, detail="Database connection failed")
    
    try:
        print(f"[upload_resume] clerk_id={clerk_id} job_posting_id={job_posting_id} filename={file.filename}")
        
        # Generate S3 key with shared storage approach
        # Store in shared folder, not job-specific folder
        s3_key = f"{clerk_id}/resumes/shared/{uuid.uuid4()}_{file.filename}"
        
        # Upload to S3
        try:
            session = aioboto3.Session()
            async with session.client(
                's3',
                aws_access_key_id=AWS_ACCESS_KEY,
                aws_secret_access_key=AWS_SECRET_KEY,
                region_name=AWS_REGION
            ) as s3_client:
                file_content = await file.read()
                await s3_client.put_object(
                    Bucket=S3_BUCKET,
                    Key=s3_key,
                    Body=file_content,
                    ContentType=file.content_type or 'application/pdf'
                )
                print(f"‚úÖ Uploaded to S3: {s3_key}")
        except Exception as e:
            print(f"‚ùå S3 upload failed: {e}")
            raise HTTPException(status_code=500, detail=f"S3 upload failed: {str(e)}")
        
        # Create resume record (stored once, referenced everywhere)
        resume_id = ResumeManager.create_resume_record(
            clerk_id, file.filename, s3_key, job_posting_id
        )
        
        # Create analysis session if job_posting_id provided
        session_id = None
        if job_posting_id:
            session_id = str(uuid.uuid4())
            cur = conn.cursor()
            try:
                # Get user
                cur.execute("SELECT id FROM users WHERE clerk_id = %s", (clerk_id,))
                user = cur.fetchone()
                
                # Create analysis session
                cur.execute("""
                    INSERT INTO analysis_sessions (
                        id, user_id, session_name, job_posting_id, 
                        status, created_at, updated_at
                    ) VALUES (%s, %s, %s, %s, 'pending', NOW(), NOW())
                """, (session_id, user[0], f"Analysis for {file.filename}", job_posting_id))
                
                # Add resume to analysis session
                cur.execute("""
                    INSERT INTO resume_analyses (resume_id, analysis_session_id, created_at)
                    VALUES (%s, %s, NOW())
                """, (resume_id, session_id))
                
                conn.commit()
                print(f"‚úÖ Created analysis session: {session_id}")
                
            except Exception as e:
                conn.rollback()
                print(f"Error creating analysis session: {e}")
            finally:
                cur.close()
        
        return {
            "message": "Resume uploaded successfully",
            "resume_id": resume_id,
            "session_id": session_id,
            "s3_key": s3_key
        }
        
    except Exception as e:
        print(f"Error in upload_resume: {e}")
        raise HTTPException(status_code=500, detail=f"Error uploading resume: {str(e)}")
    finally:
        conn.close()

@app.get("/resumes/available")
async def get_available_resumes(
    clerk_id: str,
    job_posting_id: Optional[str] = None
):
    """
    Get resumes available for analysis
    Industry approach: Proper filtering based on analysis history
    """
    try:
        resumes = ResumeManager.get_available_resumes(clerk_id, job_posting_id)
        return {
            "resumes": resumes,
            "count": len(resumes),
            "context": "general" if job_posting_id is None else f"job_{job_posting_id}"
        }
    except Exception as e:
        print(f"Error getting available resumes: {e}")
        raise HTTPException(status_code=500, detail=f"Error getting available resumes: {str(e)}")

@app.get("/resumes/{resume_id}/history")
async def get_resume_history(resume_id: str):
    """
    Get analysis history for a resume
    Industry approach: Show all contexts where resume was analyzed
    """
    try:
        history = ResumeManager.get_resume_analysis_history(resume_id)
        return {
            "resume_id": resume_id,
            "analysis_history": history,
            "total_analyses": len(history)
        }
    except Exception as e:
        print(f"Error getting resume history: {e}")
        raise HTTPException(status_code=500, detail=f"Error getting resume history: {str(e)}")

@app.post("/analysis-sessions/")
async def create_analysis_session(
    clerk_id: str = Form(...),
    session_name: str = Form(...),
    job_posting_id: Optional[str] = Form(None),
    resume_ids: str = Form(...)  # JSON string of resume IDs
):
    """
    Create analysis session and add resumes to it
    Industry approach: Many-to-many relationship
    """
    conn = get_db_connection()
    if not conn:
        raise HTTPException(status_code=500, detail="Database connection failed")
    
    try:
        cur = conn.cursor()
        
        # Get user
        cur.execute("SELECT id FROM users WHERE clerk_id = %s", (clerk_id,))
        user = cur.fetchone()
        if not user:
            raise HTTPException(status_code=404, detail="User not found")
        
        # Create analysis session
        session_id = str(uuid.uuid4())
        cur.execute("""
            INSERT INTO analysis_sessions (
                id, user_id, session_name, job_posting_id, 
                status, created_at, updated_at
            ) VALUES (%s, %s, %s, %s, 'pending', NOW(), NOW())
        """, (session_id, user[0], session_name, job_posting_id))
        
        # Parse resume IDs
        try:
            resume_id_list = json.loads(resume_ids)
        except:
            resume_id_list = [resume_ids]
        
        # Add resumes to analysis session
        added_count = 0
        for resume_id in resume_id_list:
            try:
                # Check if already exists
                cur.execute("""
                    SELECT COUNT(*) FROM resume_analyses 
                    WHERE resume_id = %s AND analysis_session_id = %s
                """, (resume_id, session_id))
                
                if cur.fetchone()[0] == 0:
                    cur.execute("""
                        INSERT INTO resume_analyses (resume_id, analysis_session_id, created_at)
                        VALUES (%s, %s, NOW())
                    """, (resume_id, session_id))
                    added_count += 1
            except Exception as e:
                print(f"Error adding resume {resume_id}: {e}")
        
        conn.commit()
        
        return {
            "message": f"Analysis session created with {added_count} resumes",
            "session_id": session_id,
            "resumes_added": added_count
        }
        
    except Exception as e:
        conn.rollback()
        print(f"Error creating analysis session: {e}")
        raise HTTPException(status_code=500, detail=f"Error creating analysis session: {str(e)}")
    finally:
        cur.close()
        conn.close()

@app.get("/analysis-sessions/{session_id}")
async def get_analysis_session(session_id: str, clerk_id: str):
    """
    Get analysis session details
    Industry approach: Show all resumes in session with their analysis results
    """
    conn = get_db_connection()
    if not conn:
        raise HTTPException(status_code=500, detail="Database connection failed")
    
    try:
        cur = conn.cursor()
        
        # Get session details
        cur.execute("""
            SELECT a.id, a.session_name, a.job_posting_id, a.status, a.created_at
            FROM analysis_sessions a
            WHERE a.id = %s AND a.user_id = (
                SELECT id FROM users WHERE clerk_id = %s
            )
        """, (session_id, clerk_id))
        
        session = cur.fetchone()
        if not session:
            raise HTTPException(status_code=404, detail="Analysis session not found")
        
        # Get resumes and their analysis results
        cur.execute("""
            SELECT 
                r.id, r.file_name, r.candidate_name, r.candidate_email, r.candidate_phone,
                ra.overall_fit_score, ra.skill_match_score, ra.project_relevance_score,
                ra.problem_solving_score, ra.tools_score, ra.summary, ra.reasoning
            FROM resume_analyses ra
            JOIN resumes r ON ra.resume_id = r.id
            WHERE ra.analysis_session_id = %s
            ORDER BY ra.overall_fit_score DESC NULLS LAST
        """, (session_id,))
        
        results = []
        for row in cur.fetchall():
            results.append({
                "resume_id": row[0],
                "filename": row[1],
                "candidate_name": row[2] or "Unknown Candidate",
                "candidate_email": row[3],
                "candidate_phone": row[4],
                "scores": {
                    "overall_fit": int((row[5] or 0) * 100),
                    "skill_match": int((row[6] or 0) * 100),
                    "project_relevance": int((row[7] or 0) * 100),
                    "problem_solving": int((row[8] or 0) * 100),
                    "tools": int((row[9] or 0) * 100)
                },
                "summary": row[10],
                "reasoning": row[11]
            })
        
        return {
            "session_id": session[0],
            "session_name": session[1],
            "job_posting_id": session[2],
            "status": session[3],
            "created_at": session[4].isoformat(),
            "results": results,
            "total_resumes": len(results)
        }
        
    except Exception as e:
        print(f"Error getting analysis session: {e}")
        raise HTTPException(status_code=500, detail=f"Error getting analysis session: {str(e)}")
    finally:
        cur.close()
        conn.close()

# Job Posting endpoints
@app.post("/job-postings/")
async def create_job_posting(
    clerk_id: str = Form(...),
    title: str = Form(None),
    description: str = Form(None),
    location: str = Form(None),
    jd_file: Optional[UploadFile] = File(None)
):
    """Create a new job posting"""
    conn = get_db_connection()
    if not conn:
        raise HTTPException(status_code=500, detail="Database connection failed")
    
    try:
        cur = conn.cursor()
        
        # Get or create user
        cur.execute("SELECT id FROM users WHERE clerk_id = %s", (clerk_id,))
        user = cur.fetchone()
        if not user:
            cur.execute("INSERT INTO users (clerk_id, created_at) VALUES (%s, NOW()) RETURNING id", (clerk_id,))
            user = cur.fetchone()
        
        # Handle JD file upload
        s3_jd_key = None
        if jd_file and jd_file.filename:
            try:
                s3_jd_key = f"{clerk_id}/job_descriptions/{uuid.uuid4()}_{jd_file.filename}"
                
                session = aioboto3.Session()
                async with session.client(
                    's3',
                    aws_access_key_id=AWS_ACCESS_KEY,
                    aws_secret_access_key=AWS_SECRET_KEY,
                    region_name=AWS_REGION
                ) as s3_client:
                    file_content = await jd_file.read()
                    await s3_client.put_object(
                        Bucket=S3_BUCKET,
                        Key=s3_jd_key,
                        Body=file_content,
                        ContentType=jd_file.content_type or 'application/pdf'
                    )
                print(f"‚úÖ Uploaded JD to S3: {s3_jd_key}")
            except Exception as e:
                print(f"‚ùå JD upload failed: {e}")
                raise HTTPException(status_code=500, detail=f"JD upload failed: {str(e)}")
        
        # Create job posting
        job_id = str(uuid.uuid4())
        cur.execute("""
            INSERT INTO job_postings (
                id, user_id, title, description, location, s3_jd_key, created_at, updated_at
            ) VALUES (%s, %s, %s, %s, %s, %s, NOW(), NOW())
        """, (job_id, user[0], title, description, location, s3_jd_key))
        
        conn.commit()
        
        return {
            "message": "Job posting created successfully",
            "job_id": job_id,
            "title": title,
            "has_jd_file": s3_jd_key is not None
        }
        
    except Exception as e:
        conn.rollback()
        print(f"Error creating job posting: {e}")
        raise HTTPException(status_code=500, detail=f"Error creating job posting: {str(e)}")
    finally:
        cur.close()
        conn.close()

@app.get("/job-postings/")
async def get_job_postings(clerk_id: str):
    """Get all job postings for a user"""
    conn = get_db_connection()
    if not conn:
        raise HTTPException(status_code=500, detail="Database connection failed")
    
    try:
        cur = conn.cursor()
        
        # Get user
        cur.execute("SELECT id FROM users WHERE clerk_id = %s", (clerk_id,))
        user = cur.fetchone()
        if not user:
            return []
        
        # Get job postings
        cur.execute("""
            SELECT id, title, description, location, s3_jd_key, created_at, updated_at
            FROM job_postings
            WHERE user_id = %s
            ORDER BY created_at DESC
        """, (user[0],))
        
        job_postings = []
        for row in cur.fetchall():
            job_postings.append({
                "id": row[0],
                "title": row[1],
                "description": row[2],
                "location": row[3],
                "has_jd_file": row[4] is not None,
                "created_at": row[5].isoformat(),
                "updated_at": row[6].isoformat()
            })
        
        return job_postings
        
    except Exception as e:
        print(f"Error getting job postings: {e}")
        raise HTTPException(status_code=500, detail=f"Error getting job postings: {str(e)}")
    finally:
        cur.close()
        conn.close()

@app.get("/job-postings/{job_id}")
async def get_job_posting(job_id: str, clerk_id: str):
    """Get a specific job posting"""
    conn = get_db_connection()
    if not conn:
        raise HTTPException(status_code=500, detail="Database connection failed")
    
    try:
        cur = conn.cursor()
        
        # Get user
        cur.execute("SELECT id FROM users WHERE clerk_id = %s", (clerk_id,))
        user = cur.fetchone()
        if not user:
            raise HTTPException(status_code=404, detail="User not found")
        
        # Get job posting
        cur.execute("""
            SELECT id, title, description, location, s3_jd_key, created_at, updated_at
            FROM job_postings
            WHERE id = %s AND user_id = %s
        """, (job_id, user[0]))
        
        job = cur.fetchone()
        if not job:
            raise HTTPException(status_code=404, detail="Job posting not found")
        
        return {
            "id": job[0],
            "title": job[1],
            "description": job[2],
            "location": job[3],
            "has_jd_file": job[4] is not None,
            "created_at": job[5].isoformat(),
            "updated_at": job[6].isoformat()
        }
        
    except Exception as e:
        print(f"Error getting job posting: {e}")
        raise HTTPException(status_code=500, detail=f"Error getting job posting: {str(e)}")
    finally:
        cur.close()
        conn.close()

@app.get("/analysis-sessions/")
async def get_analysis_sessions(clerk_id: str):
    """Get all analysis sessions for a user"""
    conn = get_db_connection()
    if not conn:
        raise HTTPException(status_code=500, detail="Database connection failed")
    
    try:
        cur = conn.cursor()
        
        # Get or create user
        cur.execute("SELECT id FROM users WHERE clerk_id = %s", (clerk_id,))
        user = cur.fetchone()
        if not user:
            cur.execute("INSERT INTO users (clerk_id, created_at) VALUES (%s, NOW()) RETURNING id", (clerk_id,))
            user = cur.fetchone()
        
        # Get analysis sessions
        cur.execute("""
            SELECT a.id, a.session_name, a.job_posting_id, a.status, a.created_at, a.updated_at,
                   j.title as job_title
            FROM analysis_sessions a
            LEFT JOIN job_postings j ON a.job_posting_id = j.id
            WHERE a.user_id = %s
            ORDER BY a.created_at DESC
        """, (user[0],))
        
        sessions = []
        for row in cur.fetchall():
            sessions.append({
                "id": row[0],
                "session_name": row[1],
                "job_posting_id": row[2],
                "status": row[3],
                "created_at": row[4].isoformat(),
                "updated_at": row[5].isoformat(),
                "job_title": row[6]
            })
        
        return sessions
        
    except Exception as e:
        print(f"Error getting analysis sessions: {e}")
        raise HTTPException(status_code=500, detail=f"Error getting analysis sessions: {str(e)}")
    finally:
        cur.close()
        conn.close()

@app.delete("/analysis-sessions/{session_id}")
async def delete_analysis_session(session_id: str, clerk_id: str):
    """Delete an analysis session and its results"""
    conn = get_db_connection()
    if not conn:
        raise HTTPException(status_code=500, detail="Database connection failed")
    
    try:
        cur = conn.cursor()
        
        # Get user first for security
        cur.execute("SELECT id FROM users WHERE clerk_id = %s", (clerk_id,))
        user = cur.fetchone()
        if not user:
            raise HTTPException(status_code=404, detail="User not found")
        
        # Get all S3 keys for resumes in this analysis session before deleting
        cur.execute("""
            SELECT r.s3_resume_key, r.file_name
            FROM resumes r
            JOIN resume_analyses ra ON r.id = ra.resume_id
            WHERE ra.analysis_session_id = %s
        """, (session_id,))
        
        s3_files_to_delete = cur.fetchall()
        print(f"[delete_analysis_session] Found {len(s3_files_to_delete)} S3 files to delete for session {session_id}")
        
        # Delete S3 files
        if s3_files_to_delete:
            try:
                session = aioboto3.Session()
                async with session.client(
                    's3',
                    aws_access_key_id=AWS_ACCESS_KEY,
                    aws_secret_access_key=AWS_SECRET_KEY,
                    region_name=AWS_REGION
                ) as s3_client:
                    for s3_key, filename in s3_files_to_delete:
                        try:
                            await s3_client.delete_object(Bucket=S3_BUCKET, Key=s3_key)
                            print(f"[delete_analysis_session] ‚úÖ Deleted S3 file: {filename} ({s3_key})")
                        except Exception as e:
                            print(f"[delete_analysis_session] ‚ùå Failed to delete S3 file {filename}: {e}")
            except Exception as e:
                print(f"[delete_analysis_session] S3 deletion error: {e}")
                # Continue with database deletion even if S3 deletion fails
        
        # Delete session (with user verification, cascade will handle resume_analyses)
        cur.execute("DELETE FROM analysis_sessions WHERE id = %s AND user_id = %s", (session_id, user[0]))
        
        if cur.rowcount == 0:
            raise HTTPException(status_code=404, detail="Analysis session not found")
        
        conn.commit()
        
        return {"message": f"Analysis session deleted successfully. Removed {len(s3_files_to_delete)} S3 files."}
        
    except Exception as e:
        conn.rollback()
        raise HTTPException(status_code=500, detail=f"Error deleting analysis session: {str(e)}")
    finally:
        cur.close()
        conn.close()

@app.post("/analysis-sessions/{session_id}/reset")
async def reset_analysis_session(session_id: str, clerk_id: str):
    """Reset an analysis session (clear results, reset status)"""
    conn = get_db_connection()
    if not conn:
        raise HTTPException(status_code=500, detail="Database connection failed")
    
    try:
        cur = conn.cursor()
        
        # Get user first for security
        cur.execute("SELECT id FROM users WHERE clerk_id = %s", (clerk_id,))
        user = cur.fetchone()
        if not user:
            raise HTTPException(status_code=404, detail="User not found")
        
        # Delete analysis results
        cur.execute("DELETE FROM resume_analyses WHERE analysis_session_id = %s", (session_id,))
        
        # Reset session status
        cur.execute("""
            UPDATE analysis_sessions 
            SET status = 'pending', updated_at = NOW()
            WHERE id = %s AND user_id = %s
        """, (session_id, user[0]))
        
        if cur.rowcount == 0:
            raise HTTPException(status_code=404, detail="Analysis session not found")
        
        conn.commit()
        
        return {"message": "Analysis session reset successfully"}
        
    except Exception as e:
        conn.rollback()
        raise HTTPException(status_code=500, detail=f"Error resetting analysis session: {str(e)}")
    finally:
        cur.close()
        conn.close()

# Resume selection endpoint
@app.post("/resumes/select")
async def select_resume(request: dict):
    """Select existing resumes for analysis"""
    conn = get_db_connection()
    if not conn:
        raise HTTPException(status_code=500, detail="Database connection failed")
    
    try:
        # Extract parameters from request
        clerk_id = request.get('clerk_id')
        s3_key = request.get('s3_key')
        job_posting_id = request.get('job_posting_id')
        
        if not clerk_id or not s3_key:
            raise HTTPException(status_code=400, detail="Missing required parameters")
        
        cur = conn.cursor()
        
        # Get user
        cur.execute("SELECT id FROM users WHERE clerk_id = %s", (clerk_id,))
        user = cur.fetchone()
        if not user:
            raise HTTPException(status_code=404, detail="User not found")
        
        # Find resume by S3 key
        cur.execute("SELECT id FROM resumes WHERE s3_resume_key = %s AND user_id = %s", (s3_key, user[0]))
        resume = cur.fetchone()
        if not resume:
            raise HTTPException(status_code=404, detail="Resume not found")
        
        resume_id = resume[0]
        
        # Create or update analysis session
        if job_posting_id:
            # Job-specific analysis
            session_name = f"Job Analysis - {job_posting_id}"
            cur.execute("""
                INSERT INTO analysis_sessions (id, user_id, session_name, job_posting_id, status, created_at, updated_at)
                VALUES (%s, %s, %s, %s, 'pending', NOW(), NOW())
                ON CONFLICT (id) DO NOTHING
            """, (str(uuid.uuid4()), user[0], session_name, job_posting_id))
        else:
            # General analysis
            session_name = "General Resume Analysis"
            cur.execute("""
                INSERT INTO analysis_sessions (id, user_id, session_name, job_posting_id, status, created_at, updated_at)
                VALUES (%s, %s, %s, NULL, 'pending', NOW(), NOW())
                ON CONFLICT (id) DO NOTHING
            """, (str(uuid.uuid4()), user[0], session_name))
        
        # Get the session ID
        cur.execute("""
            SELECT id FROM analysis_sessions 
            WHERE user_id = %s AND session_name = %s 
            ORDER BY created_at DESC LIMIT 1
        """, (user[0], session_name))
        
        session = cur.fetchone()
        if not session:
            raise HTTPException(status_code=500, detail="Failed to create analysis session")
        
        session_id = session[0]
        
        # Add resume to analysis session
        cur.execute("""
            INSERT INTO resume_analyses (resume_id, analysis_session_id, created_at)
            VALUES (%s, %s, NOW())
            ON CONFLICT (resume_id, analysis_session_id) DO NOTHING
        """, (resume_id, session_id))
        
        conn.commit()
        
        return {
            "message": "Resume selected for analysis",
            "resume_id": resume_id,
            "session_id": session_id
        }
        
    except Exception as e:
        conn.rollback()
        print(f"Error selecting resume: {e}")
        raise HTTPException(status_code=500, detail=f"Error selecting resume: {str(e)}")
    finally:
        cur.close()
        conn.close()

# S3 resume listing endpoint
@app.get("/resumes/s3/")
async def get_s3_resumes(clerk_id: str, job_posting_id: Optional[str] = None):
    """Get all available resumes from S3 and database for selection"""
    conn = get_db_connection()
    if not conn:
        raise HTTPException(status_code=500, detail="Database connection failed")
    
    try:
        cur = conn.cursor()
        
        # Get user
        cur.execute("SELECT id FROM users WHERE clerk_id = %s", (clerk_id,))
        user = cur.fetchone()
        if not user:
            return []
        
        # Get resumes from database (these are the ones that exist)
        cur.execute("""
            SELECT id, file_name, file_size, s3_resume_key, created_at, job_posting_id
            FROM resumes
            WHERE user_id = %s AND s3_resume_key IS NOT NULL
            ORDER BY created_at DESC
        """, (user[0],))
        
        db_resumes = cur.fetchall()
        print(f"[get_s3_resumes] Found {len(db_resumes)} resumes in database for user {user[0]}")
        
        # Verify resumes exist in S3 and deduplicate
        verified_resumes = {}
        try:
            session = aioboto3.Session()
            async with session.client(
                's3',
                aws_access_key_id=AWS_ACCESS_KEY,
                aws_secret_access_key=AWS_SECRET_KEY,
                region_name=AWS_REGION
            ) as s3_client:
                for resume in db_resumes:
                    resume_id, filename, file_size, s3_key, created_at, resume_job_posting_id = resume
                    
                    try:
                        # Verify file exists in S3
                        response = await s3_client.head_object(Bucket=S3_BUCKET, Key=s3_key)
                        actual_size = response['ContentLength']
                        
                        # Use filename as key for deduplication (keep the latest one)
                        if filename not in verified_resumes or created_at > verified_resumes[filename]['created_at']:
                            verified_resumes[filename] = {
                                "resume_id": resume_id,
                                "s3_key": s3_key,
                                "filename": filename,
                                "size": actual_size,
                                "created_at": created_at
                            }
                            print(f"[get_s3_resumes] Verified resume: {filename}")
                        else:
                            print(f"[get_s3_resumes] Skipping duplicate resume: {filename}")
                            
                    except Exception as e:
                        print(f"[get_s3_resumes] Resume {filename} not found in S3: {e}")
                        # Don't include resumes that don't exist in S3
                        continue
                        
        except Exception as e:
            print(f"[get_s3_resumes] S3 verification error: {e}")
            # Fallback to database-only approach
            for resume in db_resumes:
                resume_id, filename, file_size, s3_key, created_at, resume_job_posting_id = resume
                if filename not in verified_resumes:
                    verified_resumes[filename] = {
                        "resume_id": resume_id,
                        "s3_key": s3_key,
                        "filename": filename,
                        "size": file_size,
                        "created_at": created_at
                    }
        
        # Convert to expected format and filter by job analysis
        available_resumes = []
        print(f"[get_s3_resumes] Starting analysis filtering for job_posting_id: {job_posting_id}")
        
        for resume_data in verified_resumes.values():
            resume_id = resume_data["resume_id"]
            filename = resume_data["filename"]
            
            # Check if this resume has been analyzed
            if job_posting_id:
                # For specific job, check if analyzed for that EXACT job in COMPLETED sessions
                cur.execute("""
                    SELECT COUNT(*) 
                    FROM resume_analyses ra
                    JOIN analysis_sessions a ON ra.analysis_session_id = a.id
                    WHERE ra.resume_id = %s AND a.job_posting_id = %s AND a.status = 'completed'
                """, (resume_id, job_posting_id))
                
                analyzed_count = cur.fetchone()[0]
                print(f"[get_s3_resumes] Resume {filename} (ID: {resume_id}) - analyzed count for COMPLETED job {job_posting_id}: {analyzed_count}")
                
                if analyzed_count > 0:
                    print(f"[get_s3_resumes] ‚ùå EXCLUDING Resume {filename} - already analyzed for job {job_posting_id}")
                    continue
                else:
                    print(f"[get_s3_resumes] ‚úÖ INCLUDING Resume {filename} - not analyzed for job {job_posting_id}")
            else:
                # For general analysis, check if analyzed in any COMPLETED general analysis session
                cur.execute("""
                    SELECT COUNT(*) 
                    FROM resume_analyses ra
                    JOIN analysis_sessions a ON ra.analysis_session_id = a.id
                    WHERE ra.resume_id = %s AND a.job_posting_id IS NULL AND a.status = 'completed'
                """, (resume_id,))
                
                analyzed_count = cur.fetchone()[0]
                print(f"[get_s3_resumes] Resume {filename} (ID: {resume_id}) - analyzed count for COMPLETED general analysis: {analyzed_count}")
                
                if analyzed_count > 0:
                    print(f"[get_s3_resumes] ‚ùå EXCLUDING Resume {filename} - already analyzed in COMPLETED general analysis")
                    continue
                else:
                    print(f"[get_s3_resumes] ‚úÖ INCLUDING Resume {filename} - not analyzed in COMPLETED general analysis")
            
            available_resumes.append({
                "s3_key": resume_data["s3_key"],
                "filename": filename,
                "size": resume_data["size"],
                "last_modified": resume_data["created_at"].isoformat()
            })
        
        print(f"[get_s3_resumes] Returning {len(available_resumes)} available resumes")
        return available_resumes
        
    except Exception as e:
        print(f"Error getting S3 resumes: {e}")
        raise HTTPException(status_code=500, detail=f"Error getting S3 resumes: {str(e)}")
    finally:
        cur.close()
        conn.close()

# Background analysis processing
async def process_analysis_session(session_id: str):
    """Process analysis session in background"""
    conn = get_db_connection()
    if not conn:
        print(f"[process_analysis_session] Database connection failed for session {session_id}")
        return
    
    try:
        cur = conn.cursor()
        
        # Get session details
        cur.execute("""
            SELECT a.id, a.session_name, a.job_posting_id, a.user_id
            FROM analysis_sessions a
            WHERE a.id = %s
        """, (session_id,))
        
        session = cur.fetchone()
        if not session:
            print(f"[process_analysis_session] Session {session_id} not found")
            return
        
        session_id, session_name, job_posting_id, user_id = session
        
        # Update status to processing
        cur.execute("UPDATE analysis_sessions SET status = 'processing' WHERE id = %s", (session_id,))
        conn.commit()
        
        # Get job description
        job_description = get_standard_jd_template()  # Default template
        if job_posting_id:
            cur.execute("SELECT description FROM job_postings WHERE id = %s", (job_posting_id,))
            job_result = cur.fetchone()
            if job_result and job_result[0]:
                job_description = job_result[0]
        
        # Get resumes for this session
        cur.execute("""
            SELECT r.id, r.file_name, r.s3_resume_key, r.candidate_name
            FROM resume_analyses ra
            JOIN resumes r ON ra.resume_id = r.id
            WHERE ra.analysis_session_id = %s
        """, (session_id,))
        
        resumes = cur.fetchall()
        print(f"[process_analysis_session] Processing {len(resumes)} resumes for session {session_id}")
        
        # Process each resume
        for resume in resumes:
            resume_id, filename, s3_key, candidate_name = resume
            
            try:
                # Download resume from S3
                session = aioboto3.Session()
                async with session.client(
                    's3',
                    aws_access_key_id=AWS_ACCESS_KEY,
                    aws_secret_access_key=AWS_SECRET_KEY,
                    region_name=AWS_REGION
                ) as s3_client:
                    response = await s3_client.get_object(Bucket=S3_BUCKET, Key=s3_key)
                    file_content = await response['Body'].read()
                
                # Extract text
                if resume_analyzer:
                    resume_text = resume_analyzer.extract_resume_text(file_content, filename)
                    
                    # Extract candidate name if not set
                    if not candidate_name or candidate_name == "Unknown Candidate":
                        extracted_name = resume_analyzer.extract_name_from_filename(filename)
                        if extracted_name != "Unknown Candidate":
                            cur.execute("UPDATE resumes SET candidate_name = %s WHERE id = %s", (extracted_name, resume_id))
                            candidate_name = extracted_name
                else:
                    resume_text = f"Resume: {filename}"
                
                # AI Analysis
                if resume_analyzer:
                    analysis_result = await resume_analyzer.analyze_resume_with_ai(resume_text, job_description)
                else:
                    analysis_result = {
                        "Skill Match": 75,
                        "Project Relevance": 70,
                        "Problem Solving": 80,
                        "Tools": 75,
                        "Overall Fit": 75,
                        "Summary": "AI analysis not available. This is a fallback analysis."
                    }
                
                # Save analysis results
                cur.execute("""
                    UPDATE resume_analyses SET
                        overall_fit_score = %s,
                        skill_match_score = %s,
                        project_relevance_score = %s,
                        problem_solving_score = %s,
                        tools_score = %s,
                        summary = %s,
                        reasoning = %s
                    WHERE resume_id = %s AND analysis_session_id = %s
                """, (
                    analysis_result.get("Overall Fit", 75) / 100.0,
                    analysis_result.get("Skill Match", 75) / 100.0,
                    analysis_result.get("Project Relevance", 70) / 100.0,
                    analysis_result.get("Problem Solving", 80) / 100.0,
                    analysis_result.get("Tools", 75) / 100.0,
                    analysis_result.get("Summary", "No summary available"),
                    analysis_result.get("Summary", "No reasoning available"),
                    resume_id,
                    session_id
                ))
                
                print(f"[process_analysis_session] ‚úÖ Processed resume: {filename}")
                
            except Exception as e:
                print(f"[process_analysis_session] ‚ùå Error processing resume {filename}: {e}")
                continue
        
        # Update session status to completed
        cur.execute("UPDATE analysis_sessions SET status = 'completed' WHERE id = %s", (session_id,))
        conn.commit()
        
        print(f"[process_analysis_session] ‚úÖ Completed analysis session: {session_id}")
        
    except Exception as e:
        print(f"[process_analysis_session] Error: {e}")
        # Update status to failed
        try:
            cur.execute("UPDATE analysis_sessions SET status = 'failed' WHERE id = %s", (session_id,))
            conn.commit()
        except:
            pass
    finally:
        cur.close()
        conn.close()

# Start analysis endpoint
@app.post("/analysis-sessions/{session_id}/start")
async def start_analysis(session_id: str, clerk_id: str, background_tasks: BackgroundTasks):
    """Start analysis for a session"""
    conn = get_db_connection()
    if not conn:
        raise HTTPException(status_code=500, detail="Database connection failed")
    
    try:
        cur = conn.cursor()
        
        # Get user
        cur.execute("SELECT id FROM users WHERE clerk_id = %s", (clerk_id,))
        user = cur.fetchone()
        if not user:
            raise HTTPException(status_code=404, detail="User not found")
        
        # Verify session belongs to user
        cur.execute("SELECT id FROM analysis_sessions WHERE id = %s AND user_id = %s", (session_id, user[0]))
        session = cur.fetchone()
        if not session:
            raise HTTPException(status_code=404, detail="Analysis session not found")
        
        # Start background processing
        background_tasks.add_task(process_analysis_session, session_id)
        
        return {"message": "Analysis started", "session_id": session_id}
        
    except Exception as e:
        print(f"Error starting analysis: {e}")
        raise HTTPException(status_code=500, detail=f"Error starting analysis: {str(e)}")
    finally:
        cur.close()
        conn.close()

if __name__ == "__main__":
    import uvicorn
    
    # Test database connection
    conn = get_db_connection()
    if conn:
        print("‚úÖ Database connected successfully!")
        conn.close()
    else:
        print("‚ùå Database connection failed!")
        exit(1)
    
    print("üèóÔ∏è Starting Industry-Level Resume Management API...")
    print("üåê Server: http://localhost:8000")
    print("üìö API docs: http://localhost:8000/docs")
    
    uvicorn.run(app, host="0.0.0.0", port=8000)
